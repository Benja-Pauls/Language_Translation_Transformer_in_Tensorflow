{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0acb98",
   "metadata": {},
   "source": [
    "# Using Transformers for Language Translation (English to French)\n",
    "**Introduction:**\n",
    "Based off knowledge gained from analyzing the initial implementaiton of a transformer in [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf) from Google in 2017, their work can be replicated -- and even trained using ROSIE -- in order to break down the individual components and their relationships within the overall structure of the Transformer.<br/>\n",
    "The transformer architecture has been a revolutionary piece of knowledge provided to the world of sequence processing. As a seq2seq model, multiple implementations -- including modern BERT and GPT -- have used variations of this architecture to provide parallelizable throughput of input sequences, accept large input sequences with only a limit of onboard memory, and still provide generalized insights during the decoding process of the embedding space due to the introduction of self-attention.\n",
    "\n",
    "**Walkthrough by Hugging Face:** https://huggingface.co/docs/transformers/tasks/translation <br/>\n",
    "**Additional Documentation Added By:** Ben Paulson & John Cisler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820988d2",
   "metadata": {},
   "source": [
    "# Part 1: Data Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31559b",
   "metadata": {},
   "source": [
    "**Ensure all Dependencies Installed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91768f7",
   "metadata": {},
   "source": [
    "**Import All Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b714c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "import tensorflow as tf\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb13efd",
   "metadata": {},
   "source": [
    "**Import Data**<br/>\n",
    "Data includes input tensors, as well as the tokenizer used to create the tensors from the loaded corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4669c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5949c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe50146",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7128e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fa688",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83f942",
   "metadata": {},
   "source": [
    "**Batch File Run Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentionally specified by the command line (default values)\n",
    "EXP_FOLDER = None\n",
    "num_epochs = 5\n",
    "save_iterations = 10\n",
    "\n",
    "# Don't Touch -- Only for Batch Job\n",
    "IS_PYTHON = False\n",
    "\n",
    "if IS_PYTHON:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--exp_name', type=str, default='Experiment') # EXP_FOLDER\n",
    "    parser.add_argument('--epochs', type=int, default = num_epochs) # num_epochs\n",
    "    parser.add_argument('--save_iterations', type=int, default = save_iterations) # save_iterations\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    experiment_name = args.exp_name\n",
    "    num_epochs = args.epochs\n",
    "    save_iterations = args.save_iterations\n",
    "\n",
    "    # Create folder to hold sbatch runtime data\n",
    "    # if not os.path.exists('NoFileCreation'):\n",
    "    now = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\n",
    "    EXP_FOLDER = experiment_name + ' - ' + str(now)\n",
    "    os.mkdir(EXP_FOLDER)\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"RUN TIME ARGUMENTS: \")\n",
    "print(\"EXP FOLDER: \", EXP_FOLDER)\n",
    "print(\"NUM EPOCHS: \", num_epochs)\n",
    "print(\"SAVE ITERATIONS: \", save_iterations)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8781c",
   "metadata": {},
   "source": [
    "# Part 2: Metrics During Training (Evaluation Callback)\n",
    "Oultine the evaluated metrics that will be output during the training process in order to visualize the accuracy of the language-translation model without requiring inference by a human -- will produce visuals required for associated presentation in Intro to Artificial Intelligence class at MSOE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3597b22",
   "metadata": {},
   "source": [
    "**BLEU Score**<br/>\n",
    "BLEU score will be used to best match the metrics output by the \"Attention Is All You Need\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1094d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21739ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37933f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXP_FOLDER is not None:\n",
    "    class model_per_epoch(keras.callbacks.Callback):\n",
    "        \"\"\"\n",
    "        Simple callbacks class for transformer.\n",
    "        Used to save model and display loss throughout training\n",
    "        following specified save_iterations amount.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, model, filepath):\n",
    "            \"\"\"\n",
    "            Instantiating the callback for the model.\n",
    "            Outline properties to watch.\n",
    "            :param tf.Model model: Model this callback is for\n",
    "            :param str filepath: Filepath model is being saved to\n",
    "            \"\"\"\n",
    "            self.filepath=filepath\n",
    "            self.model=model\n",
    "            self.losses = [] # Compile for graphing\n",
    "            self.current_epoch = 0\n",
    "        \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            \"\"\"\n",
    "            What should occur on epoch end\n",
    "            :param int epoch: Epoch number\n",
    "            :param dictionary logs: Logs for the current training\n",
    "            \"\"\"\n",
    "            self.current_epoch = epoch\n",
    "            if epoch%save_iterations == 0: # Only save/display on save iterations\n",
    "                # Save the model at epoch\n",
    "#                 v_loss=logs.get('val_loss') \n",
    "#                 name= \"Epoch\" + str(epoch) +'-' + str(v_loss)[:str(v_loss).rfind('.')+3] + '.h5'\n",
    "#                 file_id=os.path.join(self.filepath, name)\n",
    "#                 self.model.save(file_id)\n",
    "                name = 'Transformer_Weights_Epoch' + str(self.current_epoch) + '.h5'\n",
    "                file_id = os.path.join(self.filepath, name)\n",
    "                self.model.save_weights(file_id)\n",
    "                \n",
    "                # Display a loss plot\n",
    "                self.losses.append(logs.get('val_loss'))\n",
    "                self._plot_loss()\n",
    "\n",
    "        def _plot_loss(self):\n",
    "            \"\"\"\n",
    "            Plot the loss function of compiled loss values\n",
    "            \"\"\"\n",
    "            plt.figure()\n",
    "            plt.plot(np.arange(len(self.losses)), self.losses)\n",
    "            plt.title('Training Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            \n",
    "            # Save the plot for later viewing\n",
    "            plot_name = str(self.current_epoch) + 'Epoch_Loss_Plot.png'\n",
    "            plt.savefig(EXP_FOLDER + '/' + plot_name)\n",
    "                \n",
    "    save_dir=EXP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e977f",
   "metadata": {},
   "source": [
    "# Part 3: Training the Model\n",
    "Define the model as pretrained from Hugging Face [small-t5](https://huggingface.co/t5-small) to evaluate text from English to French as trained from a paired book corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0dfc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375de3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdfa47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_books[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74877e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"my_awesome_opus_books_model\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab87fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n",
    "callbacks = [metric_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ea389",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXP_FOLDER is not None:\n",
    "    callbacks.append(model_per_epoch(model, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ac5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = list(tf_train_set)[0][0][\"input_ids\"]\n",
    "# attention_mask = list(tf_train_set)[0][0][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02372a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# # Decode the generated output tokens\n",
    "# output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee123b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5341d92",
   "metadata": {},
   "source": [
    "# Part 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=\"my_awesome_opus_books_model\")\n",
    "translator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f464c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_opus_books_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"tf\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
